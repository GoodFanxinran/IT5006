{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr7TwflWf1at"
      },
      "source": [
        "# Week 8 - Clustering (Unsupervised Learning)\n",
        "\n",
        "## Learning Objectives\n",
        "+ Hierarchical Clustering\n",
        "+ K-Means clustering\n",
        "    + Difference with hierarchical clustering\n",
        "    + Reducing the dimensions of high dimensional data\n",
        "        + PCA\n",
        "        + t-SNE\n",
        "    + Methods of finding number of clusters:\n",
        "        + Using silhoutte analysis\n",
        "        + Using elbow method\n",
        "\n",
        "The contents of this tutorial are based on the chapter 10 of \"An Introduction to Statistical Learning\" by James G. et. al., python [notebook](https://nbviewer.jupyter.org/github/JWarmenhoven/ISL-python/blob/master/Notebooks/Chapter%2010.ipynb#Lab-3:-NCI60-Data-Example) on same chapter, sklearn [tutorial](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py) on k-means, sklearn [tutorial](https://scikit-learn.org/stable/modules/clustering.html) on clustering, sklearn [tutorial](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html) on agglomerative clustering, and introduction to dimensionality reduction [tutorial](https://www.datacamp.com/community/tutorials/introduction-t-sne)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKZ93FFDf1ax"
      },
      "source": [
        "## Loading the NCI-60 dataset\n",
        "\n",
        "NCI60 is cancer cell line microarray data, which consists of 6,830 gene expression measurements on 64 cancer cell lines. So, our data is 64x6830, implying that the n<\\<p. In case of such high-dimensional data, methods such as regression do not remain applicable. Dimensionality reduction is also useful for such data, before applying different ML models. \n",
        "\n",
        "Gene data is usually always a high-dimensionality data, and is useful in precision medicine - i.e. to recommend personalized treatment to patients. A common approach for such personalized treatment is categorizing the patients into various sub-types. For such sub-typing, clustering is usually performed and can be used to identify patients who are similar - to analyze their treatments and decide the further course of treatment for the patient under consideration.  \n",
        "\n",
        "In our data, though we have cancer type label for each cell-line, in real-life data, we typically do not have this for all the patients, making the clustering quite important. Usually clustering methods are then analyzed on various datasets, and validation from the domain - such as the cancer types in our case are used to identifying a more superior technique to be used. \n",
        "\n",
        "Our data has gene expression measurement which is usually achieved by quantifying levels of the gene product, which is often a protein."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NxaydOEf1az"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y31gPczif1a0"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peV16xW7f1a0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.data.csv', skiprows=1, header=None).T\n",
        "# from pandas 0.19.2 onwards, we can directly pass the url to read_csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "zbicwPxo59Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCcNnB5if1a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "                            # Set new column names\n",
        "                            # Drop duplicated row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWSkTuwmf1a3"
      },
      "outputs": [],
      "source": [
        "labs = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.label.txt', names=['type'], dtype=\"category\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA-8Yj2rf1a4"
      },
      "outputs": [],
      "source": [
        "labs.type = labs.type.str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VMyZLXsEf1a5"
      },
      "outputs": [],
      "source": [
        "labs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5MwpIO5f1a5"
      },
      "outputs": [],
      "source": [
        "labs.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej4s4VDTf1a6"
      },
      "outputs": [],
      "source": [
        "labs.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l48ylK5Mf1a6"
      },
      "source": [
        "# Hierarchical Clustering\n",
        "\n",
        "Let us first try to find out whether or not the observations cluster into distinct types of cancer. \n",
        "\n",
        "Here we have an option to standardize the variables to have mean zero and standard deviation one. However, this step is optional and should be performed only if we want each gene to be on the same scale. We could reasonably argue that it is better not to scale genes. However, it is a choice to make as a data analyst, and also evaluate experimentally.\n",
        "\n",
        "## Agglomerative Clustering\n",
        "This is a bottom-up clustering, and is built starting from the leaves and combining clusters up to the trunk. Each dendogram will represent each of our 64 samples. As we move up the tree, some leaves begin to fuse into branches. These correspond to\n",
        "observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other. Thus, the height of the fusion depicts how different the observations are.\n",
        "\n",
        "![Agglomerative Clustering](https://dashee87.github.io/images/hierarch.gif)\n",
        "\n",
        "The [```AgglomerativeClustering```](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) class in sklearn implements four different types of linkages. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.\n",
        "+ ‘ward’ minimizes the variance of the clusters being merged.\n",
        "+ ‘average’ uses the average of the distances of each observation of the two sets.\n",
        "+ ‘complete’ or ‘maximum’ linkage uses the maximum distances between all observations of the two sets.\n",
        "+ ‘single’ uses the minimum of the distances between all observations of the two sets.\n",
        "\n",
        "The default is ward. Usually complete linkage is used for Gene data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uAZybcuf1a7"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss1YcUvcf1a8"
      },
      "source": [
        "Let us write a function to plot dendogram. We use the [```cluster.hierarchy.dendogram```](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram). The [```cluster.hierarchy```]() is available in scipy which has functions that cut hierarchical clusterings into flat clusterings or find the roots of the forest formed by a cut by providing the flat cluster ids of each observation. We will see usage of both these API - the sklearn and the scipy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWg_pSHff1a9"
      },
      "outputs": [],
      "source": [
        "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
        "\n",
        "model = model.fit(df)\n",
        "model.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALDEM1Frf1a8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "XZMiujkBf1a9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "# plot the top three levels of the dendrogram, where p is the number of levels to truncate at\n",
        "# how do we use plot_dendrogram?\n",
        "\n",
        "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ```model.children_``` attribute we use shows the children of each non-leaf node. Values less than ```n_samples``` correspond to leaves of the tree which are the original samples. A node ```i``` greater than or equal to ```n_samples``` is a non-leaf node and has children ```children_[i - n_samples]```. Alternatively at the i-th iteration, ```children[i][0]``` and ```children[i][1]``` are merged to form node ```n_samples + i```."
      ],
      "metadata": {
        "id": "3qsumGUxOEMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.children_[:5,:] "
      ],
      "metadata": {
        "id": "AmCLHNAxOBAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQItPmwUf1a9"
      },
      "source": [
        "Now we can use the scipy API and also scale the gene data and see the clusters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWFXuRl_f1a-"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster import hierarchy\n",
        "from sklearn.preprocessing import scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XkUg9T4f1a-"
      },
      "outputs": [],
      "source": [
        "X = pd.DataFrame(scale(df), index=labs.type, columns=df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can scale data using ```scale(df)``` which centers the mean and component wise scale to unit variance, along any axis. We scale such that the mean of every column is 0 and the standard deviation of every column is 1."
      ],
      "metadata": {
        "id": "KFxuUrr3ad1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how do we show that the data has been scaled?\n",
        "\n"
      ],
      "metadata": {
        "id": "cWfpdI_7aEuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "LwHBvbrzf1a-"
      },
      "outputs": [],
      "source": [
        "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(20,20))\n",
        "\n",
        "for linkage, cluster, ax in zip([hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)],\n",
        "                                ['c1','c2','c3'],\n",
        "                                [ax1,ax2,ax3]):\n",
        "    cluster = dendrogram(linkage, labels=X.index, orientation='right', color_threshold=0, leaf_font_size=10, ax=ax)\n",
        "\n",
        "ax1.set_title('Complete Linkage')\n",
        "ax2.set_title('Average Linkage')\n",
        "ax3.set_title('Single Linkage');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmcR14Qrf1a_"
      },
      "source": [
        "We see that the choice of linkage affects the results obtained. Typically, single linkage will tend to yield trailing clusters: very large clusters onto which individual observations attach one-by-one. On the other hand, complete and average linkage tend to yield more balanced, attractive clusters. For this reason, complete and average linkage are generally preferred to single linkage. \n",
        "\n",
        "In the complete linkage, we can set the threshold at 140, to get four clusters. Increasing the threshold decreases the number of clusters. For example, increasing the threshold to 150 results in only 2 clusters. This is useful in selecting the number of clusters we want. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "KI5ekxYYf1a_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,20))\n",
        "cut4 = hierarchy.dendrogram(hierarchy.complete(X),\n",
        "                            labels=X.index, orientation='right', color_threshold=140, leaf_font_size=10)\n",
        "# this plot is compatible with matplotlib. can we insert a line at the threshold of 140?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu2-374ff1a_"
      },
      "source": [
        "Based on this cut, we can see the four clusters in four colors. We see all the leukemia cell lines fall in one cluster,\n",
        "while the breast cancer cell lines are spread out over three different clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZJjMnm1f1a_"
      },
      "outputs": [],
      "source": [
        "cut4c = hierarchy.dendrogram(hierarchy.complete(X), truncate_mode='lastp', p=4,\n",
        "                             show_leaf_counts=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvUxjKMhf1bA"
      },
      "source": [
        "# K-Means for same number of clusters\n",
        "We can perform K-Means with same number of clusters and check the clusters that are formed. \n",
        "\n",
        "In K Means clustering, since we start with random choice of clusters, the results produced by running the algorithm multiple times might differ. While results are reproducible in Hierarchical clustering.\n",
        "\n",
        "![KMeans Clustering](https://miro.medium.com/max/600/1*h2WdqGZD6WsNcUdwZDqsFA.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziZvkKqlf1bA"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR1WxEA2f1bA"
      },
      "outputs": [],
      "source": [
        "np.random.seed(2)\n",
        "# we set up KMeans clustering with 4 clusters, let the algorithm run with 50 different centroid seeds. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLuYwHS8f1bA"
      },
      "outputs": [],
      "source": [
        "km4.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "NB0zTdIGf1bB"
      },
      "outputs": [],
      "source": [
        "# Observations per KMeans cluster\n",
        "pd.Series(km4.labels_).value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyPQYqhnf1bB"
      },
      "source": [
        "We see that though clustering has been done in same number of clusters - the observations per cluster is different and the clusters themselves must also be different. You can do visualizations along with the cancer types to actually see the difference between the clusters using both algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxEiXgrLf1bB"
      },
      "source": [
        "# Finding k \n",
        "Unlike the hierarchical clustering, the k-means clustering has the problem of specifying the number of clusters (k). How do we find this? We have two popular methods for this approach:\n",
        "+ Silhoutte Method\n",
        "+ Elbow Method\n",
        "\n",
        "## Silhoutte method\n",
        "Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of \\[-1, 1\\].\n",
        "\n",
        "Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
        "\n",
        "### Performing dimensionality reduction - PCA\n",
        "Sometimes performing clustering on the first few principal component score vectors can give better results than performing clustering on the full data. In this situation, we might view the principal component step as one of denoising the data. You can ofcourse perform the clustering without the PCA, however, on this specific data, the PCA helps in finding more meaningful clusters using K-means. So, let us first perform PCA on the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wxbc_41f1bB"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(random_state=rng) # default number of features to keep is the same as number of samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIcz3--Bf1bC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "wR6wiLgxf1bC"
      },
      "outputs": [],
      "source": [
        "pca.fit(df)\n",
        "\n",
        "\n",
        "# how can we see how much of the variance is explained by the principal components?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lUa6quIf1bC"
      },
      "outputs": [],
      "source": [
        "df2_plot = pd.DataFrame(pca.fit_transform(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "cIAf6Rqjf1bC"
      },
      "outputs": [],
      "source": [
        "df2_plot.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Ad05lFpRf1bD"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame([list(df2_plot.iloc[:,:5].std(axis=0, ddof=0)),\n",
        "              pca.explained_variance_ratio_[:5],\n",
        "              np.cumsum(pca.explained_variance_ratio_[:5])],\n",
        "             index=['Standard Deviation', 'Proportion of Variance', 'Cumulative Proportion'],\n",
        "             columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "u2ocaSGnf1bD"
      },
      "outputs": [],
      "source": [
        "df2_plot.iloc[:,:10].var(axis=0, ddof=0).plot(kind='bar', rot=0) # ddof is delta degree of freedom, rot is rotation for the ticks \n",
        "plt.ylabel('Variances');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HJuwlrJf1bD"
      },
      "outputs": [],
      "source": [
        "plt.xlabel('pc1')\n",
        "plt.ylabel('pc2')\n",
        "sns.scatterplot(x=df2_plot.iloc[:,0],y=df2_plot.iloc[:,1]) # we plot the first two principal components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfyMCh5bf1bD"
      },
      "source": [
        "Now once we have the principal components, the 64 dimensional data can be used to perform silhoutte analysis. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEzUOAJ5f1bD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import matplotlib.cm as cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRBPMbRyf1bD"
      },
      "outputs": [],
      "source": [
        "def silhoutte_plot(n_clusters): # from https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # The silhouette coefficient can range from -1, 1, but ranges from -0.1 to 1 for our data\n",
        "    ax.set_xlim([-0.1, 1])\n",
        "    \n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax.set_ylim([0, len(df2_plot) + (n_clusters + 1) * 10])\n",
        "\n",
        "    clusterer = KMeans(n_clusters=n_clusters, n_init=50, random_state=rng)\n",
        "    cluster_labels = clusterer.fit_predict(df2_plot)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(df2_plot, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(df2_plot, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "        \n",
        "    ax.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    plt.title((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good number of clusters is such that (1) no cluster is below the average score and (2) the clusters as proportionate as possible in size."
      ],
      "metadata": {
        "id": "D3ZaKI4UUn0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "dBllKi1qf1bE"
      },
      "outputs": [],
      "source": [
        "range_n_clusters = [2, 3, 4, 5, 6]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    silhoutte_plot(n_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGM2uFjef1bE"
      },
      "source": [
        "From the silhoutte analysis, the k=3 and k=4 seem to have good clusters, as for both the cases, the clusters are proportionate in size, and also no cluster is below the average score. Let us visualize the results for k=4 then. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "HVTe5C4Wf1bE"
      },
      "outputs": [],
      "source": [
        "kmeans_pca = KMeans(n_clusters=4, n_init=100, max_iter=400, init='k-means++', random_state=rng).fit(df2_plot)\n",
        "print('KMeans PCA Silhouette Score: {}'.format(silhouette_score(df2_plot, kmeans_pca.labels_, metric='euclidean')))\n",
        "labels_pca = kmeans_pca.labels_\n",
        "clusters_pca = pd.concat([df2_plot, pd.DataFrame({'pca_clusters':labels_pca})], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsgx4_jvf1bE"
      },
      "outputs": [],
      "source": [
        "clusters_pca.pca_clusters.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q00JbYyYf1bE"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(data=clusters_pca, x=clusters_pca.iloc[:,0], y=clusters_pca.iloc[:,1], hue='pca_clusters', palette=\"deep\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0IDw7cvf1bF"
      },
      "source": [
        "## Elbow Method\n",
        "This is another method used for selecting the number of clusters. We fit the model with range of K values. If the line chart resembles an arm, then the “elbow” (the point of inflection on the curve) is a good indication that the underlying model fits best at that point. \n",
        "\n",
        "### Performing dimensionality reduction - tSNE\n",
        "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It is extensively applied in image processing, NLP, genomic data and speech processing. In simple terms, t-SNE minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding. This is a particularly useful technique for visualizing high-dimensional data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URwGH3ZLf1bF"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmRF6eScf1bF"
      },
      "outputs": [],
      "source": [
        "# how do we apply tsne and put the results in a dataframe?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RTx3x7uf1bF"
      },
      "outputs": [],
      "source": [
        "df3.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVtEFgiDf1bG"
      },
      "source": [
        "Let us visualize this two dimensional data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNwEI2H2f1bG"
      },
      "outputs": [],
      "source": [
        "plt.xlabel('tsne1')\n",
        "plt.ylabel('tsne2')\n",
        "sns.scatterplot(x=df3.iloc[:,0], y=df3.iloc[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ypB0HCIf1bG"
      },
      "source": [
        "To find the optimal number of clusters - we plot the sum of squared distances of samples to their closest cluster center, available as ```inertia_``` attribute of the K-Means class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2w4mhgNuf1bG"
      },
      "outputs": [],
      "source": [
        "sse = []\n",
        "k_list = range(1, 20)\n",
        "for k in k_list:\n",
        "    km = KMeans(n_clusters=k, random_state=rng)\n",
        "    km.fit(df3)\n",
        "    sse.append(km.inertia_)\n",
        "    \n",
        "tsne_results_scale = pd.DataFrame({'Cluster': range(1,20), 'SSE': sse})\n",
        "tsne_results_scale.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ6j3JJsf1bH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.lineplot(data=tsne_results_scale, x='Cluster', y='SSE', marker='o')\n",
        "plt.xticks(range(1,20))\n",
        "plt.title('Optimal Number of Clusters using Elbow Method (tSNE Data)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_QDPEMVf1bH"
      },
      "source": [
        "We see that the elbow seems to be at k=4. Let us select 4 clusters and visualize. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "4YRyyolYf1bH"
      },
      "outputs": [],
      "source": [
        "kmeans_tsne_scale = KMeans(n_clusters=4, n_init=100, max_iter=400, init='k-means++', random_state=rng).fit(df3)\n",
        "print('KMeans tSNE Scaled Silhouette Score: {}'.format(silhouette_score(df3, kmeans_tsne_scale.labels_, metric='euclidean')))\n",
        "labels_tsne_scale = kmeans_tsne_scale.labels_\n",
        "clusters_tsne_scale = pd.concat([df3, pd.DataFrame({'tsne_clusters':labels_tsne_scale})], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "fgGaPUQOf1bH"
      },
      "outputs": [],
      "source": [
        "clusters_tsne_scale.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPTfObszf1bH"
      },
      "outputs": [],
      "source": [
        "clusters_tsne_scale.tsne_clusters.value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2bcZobhf1bI"
      },
      "outputs": [],
      "source": [
        "plt.xlabel('tsne1')\n",
        "plt.ylabel('tsne2')\n",
        "sns.scatterplot(data=clusters_tsne_scale, x='tsne1', y='tsne2', hue='tsne_clusters', palette='deep')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXCZ3TW4f1bI"
      },
      "source": [
        "A warning: t-SNE often fails to preserve the global geometry of the data. This means that the relative position of clusters on the t-SNE plot is almost arbitrary and depends on random initialisation more than on anything else. While this may not be a problem in some situations, scRNA-seq data sets often exhibit biologically meaningful hierarchical structure, e.g. encompass several very different cell classes, each further divided into various types. Typical t-SNE plots do not capture such global structure, yielding a suboptimal and potentially misleading visualisation. So, t-SNE may not always yield best dimensionality reduction for genetic data, however, it has shown very good results for high-dimensional data in general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blrvy6L6f1bI"
      },
      "source": [
        "### How would we analyze which clustering is most representative for this type of data?\n",
        "Usually the domain knowledge of the cancer types and which algorithm has given us good clusters which are most representative is evaluated. Another approach used for gene data is to evaluate the different patient characteristics, such as Age, co-morbidities, etc. to find if the clusters are actually identifying such patterns. Also, using multiple datasets is usually required in such unsupervised tasks. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "Tutorial 8 Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Blrvy6L6f1bI"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}