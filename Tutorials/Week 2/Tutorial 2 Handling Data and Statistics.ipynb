{"cells":[{"cell_type":"markdown","metadata":{"id":"HcwePvKpw6-3"},"source":["# Week 2 - Data and Statistics\n","\n","## Learning Objectives\n","\n","+ Mounting to Google Drive\n","+ Different Types of Data File Formats\n","  + Reading CSV file using Pandas\n","  + Introduction to Pandas\n","      + Series and Dataframes\n","+ Reading Other File Formats in Pandas\n","+ Network/Graph Data Representation\n","  + Introduction to Numpy\n","      + Array\n","  + Operations on Graph\n","      + Slicing and Broadcasting\n","+ Introduction to Scipy\n","  + Distributions\n","  + Statistical tests"]},{"cell_type":"markdown","metadata":{"id":"YBXcsmk-GhGR"},"source":["Most of the materials for this tutorial are from [pandas tutorial](https://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html), \"Python for Probability, Statistics, and Machine Learning\" by José Unpingco, [scipy lectures](https://scipy-lectures.org/packages/statistics/index.html), and [numpy tutorial](https://numpy.org/devdocs/user/whatisnumpy.html). You can refer to these resources for further understanding and practice. \n"]},{"cell_type":"markdown","source":["# Mounting your drive in Google Colab"],"metadata":{"id":"8uTFPhUWMMhX"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive') # alternative is to drag and drop to google colab"],"metadata":{"id":"UFi4kluMszoh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.insert(0,'/content/drive/My Drive/Colab Notebooks/IT5006/Week 2')\n","\n","from python_import_demo import *\n","print(imported_function(3))\n"],"metadata":{"id":"5vzen3MAMQbt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZFjLWiHGR0d"},"source":["# Different Type of Data File Formats\n","\n","In general, data can be categorized as structured and unstructured data. As Data Scientists, we come across variety of data file formats - depending on the type and modality of the data we are handling. Same data may be available to us in different formats - e.g. while working on clinical data of patients from a hospital, we can have data in any of the following formats:\n","+ Various tables (about billing and admission of patient) available with a patient-wise key mappings \\[structured data\\]\n","+ Patient-wise ECG data (time-series)\n","+ Collection of clinical notes (.txt files) for each patient\n","+ Patient-wise data for Radiology Images (image data)\n","\n","Different tools and libraries are specialized in handling certain type of data.\n","Some important file formats that we come across most often in real-life are listed below.\n","+ Tabular and Structured Data - Excel sheets, csv files, tab separated files, SQL data fall under this category\n","+ Hierarchical or Nested File Format - JSON and XML are the most popular files under this category. The big-data files of HDF5 may also fall under this category. \n","+ Image Data - The image file formats typically depend on the application\n","+ Text Data - Plain txt files are most common to store unstructured text data (which may have been scrapped from internet)\n","+ Array Data - Typically graph data, or modelling results are stored in form of array data. This may be available as NPY files or more specific binary file formats. \n","\n","In this tutorial, we will see the Python libraries available for handling a few of these data types - Pandas and Numpy. Then, we will explore the usage of another library - Scipy - to perform statistical tests using Python. "]},{"cell_type":"markdown","metadata":{"id":"Jd_rQAwIGXQP"},"source":["## Introduction to Pandas\n","\n","Pandas is a Python package which provides flexibility and expressive data structures designed to work with \"relational\" or \"labeled\" data easily. It is well suited for different kinds of data:\n","\n","+ Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet\n","\n","+ Ordered and unordered (not necessarily fixed-frequency) time series data.\n","\n","+ Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels\n","\n","+ Any other form of observational / statistical data sets. The data need not be labeled at all to be placed into a pandas data structure\n","\n","There are two primary data structures in Pandas: Series and Dataframes"]},{"cell_type":"markdown","metadata":{"id":"--SWM-0gj8MA"},"source":["## Reading csv file using Pandas\n","\n","We can read a csv into a dataframe using pandas. We now read the data contained in the ```brain_size.csv``` file. It gives the observations of brain size and weight and IQ. Reading a CSV in Pandas has many capabilities not available in the basic csv module. You can refer to the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) for all the various parameters. "]},{"cell_type":"markdown","source":["Please note that the community agreed alias for pandas is pd, so loading pandas as pd is assumed standard practice for all the pandas documentation."],"metadata":{"id":"O285QFy3lnJ5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5iDCf5XVlKHt","scrolled":true},"outputs":[],"source":["import pandas as pd\n","\n","data_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IT5006/Week 2/brain_size.csv\", sep=';', na_values='.', index_col=0)\n","data_df"]},{"cell_type":"markdown","metadata":{"id":"9W9eQ6SwGR0e"},"source":["We have explored the usage of the most popular file format for tabular data - CSV (comma separated values) file. A CSV file is essentially a text file only, but the various columns are separated by comma, and every new line represents a new row. We can read a CSV file like a normal python file, and split on comma, or even use the natively available [csv module](https://docs.python.org/3/library/csv.html) for reading and writing CSV files.\n","\n","However, for performing analysis, and further integrating with typical ML pipeline, Pandas library is typically used for Data Analytics. "]},{"cell_type":"markdown","source":["## Reading text file using Pandas\n","\n","We can also read a txt file into a dataframe using the versatile ```read_csv``` function in pandas. We now read the data contained in the ```earth_orbital_data.lst``` file. It gives the location of the Earth with respect to the sun (the radius, latitude and longitude) on various days in the year 2000. We then change the column names so they are easier to understand. The data was retrieved from NASA: ```https://omniweb.gsfc.nasa.gov/coho/helios/heli.html```"],"metadata":{"id":"A7YR6reSjX9E"}},{"cell_type":"code","source":["earth_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IT5006/Week 2/earth_orbital_data.lst\", sep = \" \", skipinitialspace = True)\n","earth_df = earth_df.rename(columns={'YEAR': 'Year', 'DAY': 'Day', \"RAD_AU\": \"Radius\", \"SE_LAT\": \"Latitude\", \"SE_LON\": \"Longitude\"})\n","earth_df"],"metadata":{"id":"lt-STOsHj8Uq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using pandas"],"metadata":{"id":"A8VpvFxUlvgp"}},{"cell_type":"markdown","metadata":{"id":"jFa09yIvIT2D"},"source":["### Series\n","\n","Series objects combines an index and corresponding data values. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASUG8o5pJC2o"},"outputs":[],"source":["x = pd.Series(index = range(1, 11), data = [x**2 for x in range(1,11)], name=\"Squares\")\n","x"]},{"cell_type":"markdown","source":["We can retrieve values from a dataframe using ```iloc```, which is an integer-location based indexing that is used for selection by position."],"metadata":{"id":"sWQ-5271mBjT"}},{"cell_type":"code","source":["x.iloc[5]"],"metadata":{"id":"SeMNTj8crzaT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qd_L5HnlJebF"},"outputs":[],"source":["x.iloc[1:3]"]},{"cell_type":"markdown","metadata":{"id":"ONLqdibmJ8Mf"},"source":["### Dataframe\n","\n","Formally, we refer to the two-dimensional, potentially heterogeneous, tabular data structure as a Dataframe. It can be thought of as dictionary-like container for Series objects. It is similar to a spreadsheet and can also be loaded from csv files.\n","\n","<img src = \"https://pandas.pydata.org/pandas-docs/stable/_images/01_table_dataframe.svg\">\n","\n","Every column in a dataframe is a Series. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"waemnge6jUBk"},"outputs":[],"source":["cubes = pd.Series(index=range(1,11), data=[x**3 for x in range(1,11)], name=\"Cubes\")\n","df = pd.DataFrame({\"Squares\":x, \"Cubes\":cubes})\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RUjBjrAj4Zz"},"outputs":[],"source":["type(df['Squares'])"]},{"cell_type":"markdown","metadata":{"id":"gjApR1tfGR0j"},"source":["We can see that the datatypes for the dataframe have been inferred. Columns with mixed types are stored with the object dtype. We can also set the dtype to be [categorical](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html) or [datetime](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html). You can read the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#basics-dtypes) for the various data types."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6nmAKldGR0j"},"outputs":[],"source":["data_df.dtypes"]},{"cell_type":"markdown","source":["How can we convert the ```Gender``` column with mixed types into categorical data?"],"metadata":{"id":"BRPsY7a4oDsz"}},{"cell_type":"code","source":[""],"metadata":{"id":"IP345az_Rw8M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FHisLE5YlZfR"},"source":["### Operations on Dataframes\n","\n","We can get some quick properties of the data using Dataframe functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRktudv8pzD6"},"outputs":[],"source":["data_df.head(10)"]},{"cell_type":"markdown","metadata":{"id":"haUem1qYp--f"},"source":["We see that the second row in the dataframe has NaN value in \"Weight\" column - which indicates a missing value. Note that it is important to handle these missing values when doing statistical analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwLjdUISpsAm"},"outputs":[],"source":["data_df.shape"]},{"cell_type":"markdown","metadata":{"id":"q1LiFiG0qekN"},"source":["This indicates that the dataframe has 40 rows and 7 columns. We can see the names of the columns if present in the csv too."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GvLlonxqY8J"},"outputs":[],"source":["data_df.columns"]},{"cell_type":"markdown","metadata":{"id":"K1azqc4Wqr42"},"source":["Pandas provides us with a quick way to generate descriptive statistics using the ```describe``` function. We will get the descriptive statistics for each of the numeric columns in the data frame.\n","\n","These statistics are generated by excluding the missing values in the data. The count value in the statistics can also thus be used to get an idea of the number of missing values in the column. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URC00lQVqaqX"},"outputs":[],"source":["data_df.describe()"]},{"cell_type":"markdown","metadata":{"id":"gzcWeZH1sUi2"},"source":["We can also check the dataframe for a specific value of specific column directly using format ```df[col]==val```. This returns a series with ```True``` values for only those rows which have the specific ```val``` in the ```col``` mentioned. \n","\n","How would we find out which rows correspond to observations of female patients?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQ1KQTL-s0aC"},"outputs":[],"source":["df_female = data_df['Gender']==\"Female\"\n","df_female.head()"]},{"cell_type":"markdown","metadata":{"id":"nTu0yrIRtGiR"},"source":["We can use the series to filter the dataframe to get only the rows which adhere to the condition mentioned. We can extract further information from the filtered rows.\n","\n","What is the mean VIQ of female patients?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VTC0jgXVqe2W"},"outputs":[],"source":["data_df[data_df['Gender']==\"Female\"]['VIQ'].mean()"]},{"cell_type":"markdown","metadata":{"id":"HyNbbOY32ZlE"},"source":["Alternatively, we can use the series to create a new column is Female too."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ADPMW_gZ2e3v"},"outputs":[],"source":["data_df[\"isFemale\"] = df_female\n","data_df.head()"]},{"cell_type":"markdown","source":["How do we find the mean VIQ of female patients with weight greater than 120 pounds?"],"metadata":{"id":"EJUtRLfXsGUd"}},{"cell_type":"code","source":[""],"metadata":{"id":"iETr4PiqR1pc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oCJCeQIaq-hC"},"source":["#### Grouping on values \n","\n","You can use ```groupby``` function to split the dataframe on values of the variable.\n","\n","We can view the VIQ of all females in the dataset by grouping the patients by gender."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xlb8UTQvrHTO"},"outputs":[],"source":["gender_groupby = data_df.groupby(\"Gender\")\n","for gender, val in gender_groupby[\"VIQ\"]:\n","  print(\"Gender:\"+str(gender))\n","  print(\"Values\")\n","  print(val)"]},{"cell_type":"markdown","metadata":{"id":"bUlmYwpno2Mx"},"source":["So, we see that groupby returns an object that contains information for the group - we created a series grouped on the value of Gender. \n","\n","Typically we use groupby to get some aggregate statistics for the groups we have done a groupby on. It is important to note that groupby evaluation is lazy - i.e. no computation is done until the aggregation function is applied. \n"]},{"cell_type":"code","source":["gender_groupby"],"metadata":{"id":"Fb3QgJSnw3kK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For example, can find the mean FSIQ, VIQ, PIQ, weight, height and MRI count among females and males using groupby."],"metadata":{"id":"cI0W8ZVxw328"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCJZt-GgvIRk","scrolled":true},"outputs":[],"source":["gender_groupby.mean()"]},{"cell_type":"markdown","metadata":{"id":"i3OcrAvOvsNx"},"source":["How many males/females are included in the study?"]},{"cell_type":"code","source":[""],"metadata":{"id":"xUMPNnF6DQy3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What is the mean value for VIQ for entire population? Also, print the mean VIQ for each gender in the following format:\n","```\n","Male, VIQmeanvalue\n","Female, VIQmeanvalue\n","```\n"],"metadata":{"id":"QvbpVR90DNLn"}},{"cell_type":"code","source":[""],"metadata":{"id":"Wtv3PBaGD3b_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a new column named BMI using the Weight and Height column. The formula is weight(in kgs)/square of height(in m). The height is in inches and weight is in lbs. You can create two new columns with appropriate units before making BMI column. \n"],"metadata":{"id":"QWp-83ZmDOQA"}},{"cell_type":"code","source":["data_df.head(5)"],"metadata":{"id":"B1Vf7DGEEZr3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KCC3OvI_EXpn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How does BMI vary among males and females (variation according to underweight(under 18.5), normal(18.5-22.9), and overweight(above 22.9))? "],"metadata":{"id":"xjegN2IFDPLZ"}},{"cell_type":"code","source":[""],"metadata":{"id":"6uHtvoMxMuQR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLY1i3omGR0n"},"source":["## Handling Other File Formats in Pandas\n","\n","Pandas supports reading many different file formats - including general delimited file (which is referred to as table), Excel, JSON, HDF5, SQL, etc. You can refer to the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/io.html) for full details. \n","\n","### JSON files\n","In Python, JSON files can natively also be handled using the [json](https://docs.python.org/3/library/json.html) module. We basically treat the data as Python dictionary. \n","\n","Let us quickly explore handling of JSON file in Pandas. For reading a JSON file, the orientation of the file plays an important role, which describes how the JSON file is structured and must be read.  \n","+ 'split' : dict like {index -> \\[index\\], columns -> \\[columns\\], data -> \\[values\\]}\n","+ 'records' : list like \\[{column -> value}, ... , {column -> value}\\]\n","+ 'index' : dict like {index -> {column -> value}}\n","+ 'columns' : dict like {column -> {index -> value}}\n","+ 'values' : just the values array\n","\n","We can write the `data_df` dataframe as JSON and read it. "]},{"cell_type":"code","source":["exchange_rates_df = pd.read_json(\"/content/drive/MyDrive/Colab Notebooks/IT5006/Week 2/exchangerates.json\", orient = \"records\")\n","exchange_rates_df"],"metadata":{"id":"sQ6DsjsO-lzH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hYwYH1wnGR0n"},"source":["# Handling Network/Graph Data\n","\n","Suppose we are working on social network data. Let us consider we have the adjacency matrix representation of the social network connectivity available.   \n","\n","The adjacency matrix indicates the connectivity between two nodes, each represented by the index of the row and column of the matrix. The presence of an edge between the nodes is indicated by 1 (or the weight in case of weighted graph) or 0 otherwise. \n","\n","\n","| ![adj_mat.jpg](https://mathworld.wolfram.com/images/eps-gif/AdjacencyMatrices_1002.gif) | \n","|:--:| \n","| Image source: Wolfram MathWorld |\n","\n","Rather than representing this graph data as list of lists using Python, we can use another library - Numpy to represent the adjacency matrix and perform compuattion easily. Incidentally, Pandas is built on top of Numpy, and so all the Pandas operations like sum, aggregation, etc. use the NumPy functionality."]},{"cell_type":"markdown","metadata":{"id":"ngqcFt6xrZ89"},"source":["## Introduction to Numpy\n","\n","Numpy is a fundamental package for scientific computing in Python. At the core of a numpy package is *ndarray* object. This is encapsulation of *n*-dimensional arrays of **homogeneous** data types. There are many important differences between numpy arrays and standard Python sequences:\n","+ Numpy arrays have fixed size at creation.\n","+ The items have to be of homogeneous data type.\n","+ Numpy arrays efficiently execute advanced mathematical operations. \n","\n","### Array\n","\n","We can create a numpy array from a homogeneous list. Numpy's array class is called [```ndarray```](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html). It is also known by the alias ```array```. The ```ndarray``` has attributes like ```ndim```, ```shape```, ```size```, ```dtype```, ```itemsize```, ```data```, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mY5SxSVwvRti"},"outputs":[],"source":["import numpy as np\n","\n","a = np.array([1,2,5,3])\n","print(a)\n","\n","b = np.array([\"This\", \"is\", \"a\", \"np\", \"array\", \"of\", \"strings\"])\n","print(b)\n","\n","b.shape"]},{"cell_type":"markdown","metadata":{"id":"jIgeJfHrGR0o"},"source":["We have many options to create new numpy arrays. Let us quickly go through each of the different ways, other than ```np.array```."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnM716NyGR0o"},"outputs":[],"source":["c = np.ones((3,4))\n","d = np.full((3,4), 0.15)\n","e = np.empty((3,2))\n","\n","print(c)\n","print(d)\n","print(e)"]},{"cell_type":"markdown","metadata":{"id":"U1RweEsCGR0o"},"source":["We can use usual element-wise operations on these arrays."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yoHZaZj0GR0o"},"outputs":[],"source":["print(c+d)\n","print(c*d)"]},{"cell_type":"markdown","metadata":{"id":"buv2z0sqaMg4"},"source":["```np.arange``` returns evenly spaced values within a given interval - with start, stop and step size which can be specified. Another function which can be used for getting evenly spaced values in specific interval - [```np.linspace```](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html). Rather than step-size as used in arange, it uses number of samples to generate.\n","\n","Another function which is handy is ```reshape``` - it can change the shape of the array without changing its data. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xRL2I-LZ_8J","scrolled":true},"outputs":[],"source":["a = np.arange(1, 25, 3)\n","print(a)"]},{"cell_type":"code","source":["a1 = a.reshape(4,2)\n","a1"],"metadata":{"id":"hYazdwJspwVO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can think of the reshaping process as reading across rows first, then across columns."],"metadata":{"id":"sgVig_PRqHNb"}},{"cell_type":"code","source":["a1.reshape(2,4)"],"metadata":{"id":"IzvVWh7tpxs7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BfsBWHDQGR0p"},"source":["## Operations on Adjacency Matrix using Numpy\n","\n","Let us use the adjacency matrix for a 4 node graph shown in the figure earlier.\n","```\n","[[0 0 1 1]\n","[0 0 1 1]\n","[1 1 0 0]\n","[1 1 0 0]]\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTUS8LDHGR0p"},"outputs":[],"source":["c = np.array([[0,0,1,1],[0, 0, 1, 1],[1, 1, 0, 0],[1, 1, 0, 0]])\n","c"]},{"cell_type":"markdown","metadata":{"id":"_os8A5VLGR0p"},"source":["Now, if we want to find the degree of each node, we need to sum the number of edges for each node. We can use the Numpy operation ```sum``` for finding the degree of each node. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-O7aPDEGR0p"},"outputs":[],"source":["c.sum(axis=0) # sum over each column"]},{"cell_type":"markdown","metadata":{"id":"3fCLO-8cGR0q"},"source":["So, we see that the degree of each node is 2. Numpy allows other reductions as well - such as ```argmin```. For these operations, we need to specify the axis for our operation. If the axis is not mentioned, the returned index is into the flattened array. For a visual representation, the following image helps capture the direction for operation on the array.\n","\n","| ![axis.jpg](https://i.stack.imgur.com/h1alT.jpg) | \n","|:--:| \n","| Image source: Stack Overflow |"]},{"cell_type":"markdown","metadata":{"id":"bcWn9dqwGR0q"},"source":["We can similarly get statistical results on the data too, again specifying the axes. We can get mean, standard deviation, and many more statistics, using this similar principle. You can look up the documentation to get complete list of functionality. You can explore all such Numpy operations in the [documentation](https://numpy.org/doc/stable/reference/routines.statistics.html)."]},{"cell_type":"markdown","metadata":{"id":"7zRAVAq6GR0q"},"source":["### Slicing and Broadcasting\n","\n","Slicing, indexing and iteration in one-dimensional array are much similar to lists in python. Multidimensional arrays can have one index per axis. When fewer indices are provided than the number of axes of the array, the missing indices are considered as complete slices."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIh3EoLAGR0q"},"outputs":[],"source":["print(c[2:4,:])\n","print(c[2:4])"]},{"cell_type":"markdown","metadata":{"id":"oDAlcvoGGR0q"},"source":["We have seen that operations are usually performed element-wise, when both the arrays are of the same shape. Numpy’s broadcasting rule relaxes this constraint when the array shapes meet certain constraints. \n","\n","When operating on two arrays, numpy compares their shapes. It starts with the rightmost dimension and works its way left. Two dimensions are compatible when\n","1. they are equal, or\n","2. one of them is 1\n","\n","Note that this implies that the values do not need to same number of dimensions!"]},{"cell_type":"markdown","metadata":{"id":"e-fq0E3WGR0r"},"source":["#### Application of Broadcasting - Creating distance matrix\n","\n","Let’s construct an array of distances (in miles) between cities of Route 66 in US: Chicago, Springfield, Saint-Louis, Tulsa, Oklahoma City, Amarillo, Santa Fe, Albuquerque, Flagstaff and Los Angeles. \n","\n","```\n","[0, 198, 303, 736, 871, 1175, 1475, 1544, 1913, 2448]\n","```\n","\n","We can create the adjacency matrix for these cities assuming that the only route for travel is the route mentioned. We will use ```np.newaxis``` for creating column vector from the np array. Then using the broadcasting technique, we can compute the adjacency matrix between the cities. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmdOW6VkGR0r"},"outputs":[],"source":["mileposts = np.array([0, 198, 303, 736, 871, 1175, 1475, 1544, 1913, 2448])\n","mileposts[:, np.newaxis] # or np.transpose([mileposts])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwu_E-0AGR0r"},"outputs":[],"source":["distance_array = np.abs(mileposts - mileposts[:, np.newaxis])\n","distance_array"]},{"cell_type":"markdown","metadata":{"id":"bTZqgqXXGR0r"},"source":["Using the adjacency matrix, you can do various computations like shortest path between two nodes, etc. These fall into the category of graph algorithms and are beyond scope of the course, but representing adjacency matrix as numpy array gives the flexibility of performing quick operations."]},{"cell_type":"markdown","source":["### Generating random arrays"],"metadata":{"id":"0NlIH7y_szrl"}},{"cell_type":"code","source":[" r = np.random.rand(10,2)\n"," r"],"metadata":{"id":"adIw1nGms5oD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How can I find the maximum value in the array?"],"metadata":{"id":"lh-PnRyds_LU"}},{"cell_type":"code","source":["np.max(r)"],"metadata":{"id":"SIFJ9vd2s-fE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How can I find the location of the maximum value in the array?"],"metadata":{"id":"Bprsij3btNLD"}},{"cell_type":"code","source":[""],"metadata":{"id":"AALl94UPtQB7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How do conditionals work on an array?"],"metadata":{"id":"HOCODKHQ4Pmb"}},{"cell_type":"code","source":["if r > 0.8:\n","  print(\"All values are larger than 0.8\")\n","else:\n","  print(\"Some values in r are smaller than 0.8\")"],"metadata":{"id":"VQPZ7wW94O91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"aKkax_HW4u6y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"423uG_WsGR0s"},"source":["Now that we have seen the various types of data available, let us understand the implementation of statistical inference using Python. We will use the library Scipy for the same. \n","\n","# Introduction to Scipy\n","\n","The scipy package contains various toolboxes dedicated to common issues in scientific computing. Its different submodules correspond to different applications, such as interpolation, integration, optimization, image processing, statistics, special functions, etc. \n","\n","Although there are some basic statistical functions in numpy (e.g., mean, std, median), the real repository for statistical functions is in ```scipy.stats```. There are over eighty continuous probability distributions implemented in ```scipy.stats``` and an additional set of more than ten discrete distributions, along with many other supplementary\n","statistical functions.\n","\n","We will go through these distributions and also do basic hypothesis testing using ```scipy.stats```."]},{"cell_type":"markdown","metadata":{"id":"GK98NTRUGR0s"},"source":["## Distributions\n","\n","Given observations of a random process, their histogram is an estimator of the random process’s PDF (probability density function). We can use ```np.random``` for generating random samples. \n","\n","However here, we continue working with the data we have loaded, and fit distribution to the VIQ column of the data. We assume that the data for VIQ is drawn from normal distribution. Let us convert the VIQ column to numpy array first."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2OvtKQ0GR0s"},"outputs":[],"source":["samples = data_df.VIQ.to_numpy() # convert series to np array\n","samples"]},{"cell_type":"markdown","metadata":{"id":"uXUgqJHiGR0s"},"source":["We can now fit a normal distribution to this observed data. We do a maximum-likelihood fit of the observations to estimate the parameters of the underlying parameters. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbwcamXaGR0s"},"outputs":[],"source":["from scipy import stats\n","\n","loc, std = stats.norm.fit(samples)\n","print(loc)\n","print(std)"]},{"cell_type":"markdown","metadata":{"id":"wt7acg3jGR0s"},"source":["The mean is an estimator of the center of the distribution. The normal distribution distribution fit by scipy should give same have same center as the mean of the sample as we have created a sample distribution. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6uz5mzTGR0t"},"outputs":[],"source":["np.mean(samples)"]},{"cell_type":"markdown","metadata":{"id":"Khcr2SfVGR0t"},"source":["The median is another estimator of the center. It is the value with half of the observations below, and half above. It is the 50th percentile. Unlike the mean, the median is not sensitive to the tails of the distribution. It is “robust”."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uNK4VbZOGR0t"},"outputs":[],"source":["np.median(samples)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1_VUyn_GR0t"},"outputs":[],"source":["stats.scoreatpercentile(samples, 50)"]},{"cell_type":"markdown","metadata":{"id":"GbEUbei9GR0u"},"source":["## Statistical tests using Scipy\n","\n","A statistical test is a decision indicator. For instance, if we have two sets of observations, that we assume are generated from Gaussian processes, we can use a T-test to decide whether the means of two sets of observations are significantly different. "]},{"cell_type":"markdown","metadata":{"id":"6n719EyGGR0u"},"source":["Let us begin our hypothesis testing using the initial exmaple of brain data. Consider the null hypothesis that the expected value (mean) of the sample of VIQ values is equal to the given population mean. The ```ttest_1samp``` function does two-sided test and returns the T-statistic and p-value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-VGZ2kgjGR0u"},"outputs":[],"source":["stats.ttest_1samp(samples, 0) # population mean is 0"]},{"cell_type":"markdown","metadata":{"id":"-Op6R_VCGR0u"},"source":["Let the significance value be 0.05. As p-value<0.05, we can reject the null hypothesis that population mean for the VIQ measure is 0."]},{"cell_type":"markdown","metadata":{"id":"MzT_j4NEGR0u"},"source":["We have already seen that the mean for VIQ is different for males and females. So we can perform a test to check if this difference is significant. We use ```ttest_ind``` function for doing a two-sided test for the null hypothesis that the two independent samples have identical average (expected) values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKdz3lClGR0u"},"outputs":[],"source":["female_viq = data_df[data_df['Gender'] == 'Female']['VIQ']\n","male_viq = data_df[data_df['Gender'] == 'Male']['VIQ']\n","stats.ttest_ind(female_viq, male_viq)"]},{"cell_type":"markdown","metadata":{"id":"RpS-b2tbGR0v"},"source":["As the p-value>0.05, we can not reject the null hypothesis that the males and females have identical VIQ measure. "]},{"cell_type":"markdown","source":["#### Z-tests using Scipy\n","\n","If the mean and standard deviation of a population is known, we can use the z-test. We can use the lightbulb example in the lecture slides.\n","\n","A particular brand of tires claims that its deluxe tire averages at least 50,000 km before it needs to be replaced. From past studies of this tire, the standard deviation is known to be 8,000 km. A survey of owners of that tire design is conducted. From the 28 tires surveyed, the mean lifespan was 46,500 km with a standard deviation of 9,800 km. Using α=0.05, is the data highly inconsistent with the claim?"],"metadata":{"id":"8jzk6btj9lR0"}},{"cell_type":"markdown","source":["$H_0: \\mu \\geq 50,000$\n","\n","$H_1: \\mu < 50,000$\n","\n","$z = \\frac{50000-46500}{8000/\\sqrt{28}} = 2.315$\n","\n","How can we calculate the p-value?"],"metadata":{"id":"-_UiiRC3VjKP"}},{"cell_type":"code","source":["z = (50000-46500)/(8000/np.sqrt(28))"],"metadata":{"id":"Wmy4K7S0AX6L"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Tutorial 2 Handling Data and Statistics.ipynb","provenance":[{"file_id":"1htXNS-lBJIeE0X2m_wK7lfjuayvWv6Wc","timestamp":1642695072183}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}