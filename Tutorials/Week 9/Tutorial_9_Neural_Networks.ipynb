{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 9 Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks for Supervised and Unsupervised Training (Regression)\n",
        "+ Pendulum Dataset\n",
        "+ Supervised Learning using a Neural Network\n",
        "    + Setting up a Neural Network\n",
        "+ Physics knowledge of the Hamiltonian System\n",
        "    + Unsupervised Learning using a Neural Network"
      ],
      "metadata": {
        "id": "F06dDNkn0HG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Simple Pendulum\n",
        "\n",
        "![A simple pendulum. Its position is defined by $\\theta$ and the velocity is defined as $v$](https://upload.wikimedia.org/wikipedia/commons/thumb/2/24/Oscillating_pendulum.gif/300px-Oscillating_pendulum.gif)\n",
        "\n",
        "We consider a pendulum with mass 1kg and the length of the massless cord is 1m. It swings freely back and forth. \n",
        "\n",
        "The rate of change of the position of the pendulum is $\\frac{d\\theta}{dt}$ and the rate of change of the acceleration of the pendulum is $\\frac{dv}{dt}$. The acceleration of the pendulum is also $\\frac{d^2 \\theta}{dt^2} = -\\sin\\theta$. You can refer to [this website](https://www.acs.psu.edu/drussell/Demos/Pendulum/Pendula.html#:~:text=A%20simple%20pendulum%20consists%20of,and%20forth%20with%20periodic%20motion.) for more details about the simple pendulum.\n",
        "\n"
      ],
      "metadata": {
        "id": "I1QJ95pS1BKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/zykhoo/IT5006.git"
      ],
      "metadata": {
        "id": "uzlawanU3j6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import our dataset of (left to right, by column):\n",
        "+ randomly sampled positions within the range of $[-2\\pi, 2\\pi]$, $\\theta_1$\n",
        "+ randomly sampled velocities within the range of $[-1.2,1.2]$, $v_1$\n",
        "+ positions a small time step of $t=0.1$ later, $\\theta_2$\n",
        "+ velocities a small time step of $t=0.1$ later, $v_2$\n",
        "+ the derivative of each position, $\\frac{\\theta2 -\\theta_1}{t}$\n",
        "+ the derivative of each velocity, $\\frac{v_2-v_1}{t}$\n"
      ],
      "metadata": {
        "id": "B4KOTcSPGMq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/IT5006/pq_data.txt\", sep = ',', header = None)\n",
        "df.columns = [\"theta\",\"v\",\"dtheta\",\"dv\"]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "lF4nxo213MdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Neural Network\n",
        "\n",
        "We want to use a neural network to take in $\\theta$ and $v$, and return the rate of change of the position and velocity. $NN(\\theta,v)=(\\frac{d\\theta}{dt},\\frac{dv}{dt})$.\n",
        "\n",
        "This is a **supervised learning** example because we know the rate of change of the position and velocity.\n",
        "\n",
        "The neural network is basically approximating the functions $\\frac{d\\theta}{dt} = v$ and $\\frac{dv}{dt} = -\\sin(\\theta)$. However, it does not know the equation, but instead uses several layers of perceptrons to approximate these equations. The neural network is a **black box model**.\n",
        "\n",
        "We can use a neural network to approximate these functions (and even more complex functions!) because neural networks are [universal function approximators](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208). This applies to any neural network with at least one hidden layer. "
      ],
      "metadata": {
        "id": "H9tMv8EK2tBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "KBdQ7LCyIO9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a neural network with one input layer, and three hidden layers (inclusive of the output layer). Each hidden layer has 16 neurons, and the output layer has 2 neurons, as there are 2 outputs $\\frac{d\\theta}{dt}$ and $\\frac{dv}{dt}$. "
      ],
      "metadata": {
        "id": "Ax80UIR7Io01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "def softplus(x):\n",
        "    return torch.log(torch.exp(x)+1)\n",
        "\n",
        "class SoftPlus(nn.Module):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Init method.\n",
        "        '''\n",
        "        super().__init__() # init the base class\n",
        "\n",
        "    def forward(self, x):\n",
        "        return softplus(x)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net , self).__init__()\n",
        "        self.hidden_layer_1 = nn.Linear( input_size, hidden_size, bias=True)\n",
        "        self.hidden_layer_2 = nn.Linear( hidden_size, hidden_size, bias=True)\n",
        "        self.output_layer = nn.Linear( hidden_size, output_size , bias=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = softplus(self.hidden_layer_1(x)) # F.relu(self.hidden_layer_1(x)) # \n",
        "        x = softplus(self.hidden_layer_2(x)) # F.relu(self.hidden_layer_2(x)) # \n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "2assL53FIAqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's create the neural network. how many layers does it have?\n"
      ],
      "metadata": {
        "id": "S9g2UyIbPtbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to choose the device to run our neural network on, and store our data. We make use of the context-manager that changes the selected device, ```torch.device()```. We use 'cpu' if we run our neural network on our CPU, and 'cuda' if we run our neural network on our GPU.\n",
        "\n",
        "When do we use GPUs? Training neural networks is a hardware intensive task which requires many small numerical calculations. A decent GPU will make sure these computations go smoothly. For the purpose of this tutorial, we do not need to use a GPU because our neural network is small and there are fewer numerical calculations. The CPU suffices. However, for other tasks such as image recognition or natural language processing, a GPU can significantly improve performance.  "
      ],
      "metadata": {
        "id": "HBuNDVsocdUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cpu')\n",
        "torch.manual_seed(1)\n",
        "torch.cuda.manual_seed_all(1)"
      ],
      "metadata": {
        "id": "onIkTDzvKJD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Creation\n",
        "\n",
        "Let's create the matrix that we use for our neural network. This matrix comprises the input and the groundtruth output of the neural network, for supervised learning. We make use of sklearn's train_test_split to split this dataset into a training and validation dataset. The training dataset is used to train the neural network. The validation data set provides an unbiased evaluation of a model fit on the training data set."
      ],
      "metadata": {
        "id": "i1HR9avuL0As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mat = df.to_numpy()\n",
        "mat = torch.from_numpy(mat)\n",
        "mat = mat.to(device)\n",
        "\n",
        "# split the dataset into training and validation sets\n"
      ],
      "metadata": {
        "id": "orlUCtg_JMUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "\n",
        "The optimizer we use is the [Adam optimizer](https://paperswithcode.com/method/adam), which is adaptive and gradually decreases the learning rate as the number of epochs increase."
      ],
      "metadata": {
        "id": "vgHqPJqWQStU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "NX_5rVD9Itmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Neural Network\n",
        "\n",
        "We train the neural network for 1600 epochs and realise the validation loss has stagnated. We observe this by printing the validation loss every 10 epochs. \n",
        "\n",
        "As our dataset is small, we can make use of the entire dataset when training the neural network in each epoch. If the dataset is larger, we can split the dataset into batches and train one batch at a time per each epoch.\n",
        "\n",
        "For every epoch, we feed the inputs into the neural network, and calculate the mean squared error loss between the output of the neural network and the groundtruth. We then backpropagate through the neural network using this loss, and update the weights of the neural network using Adam and the loss. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mB7Bxqf1k0tV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1600):  # loop over the dataset multiple times\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # for count in range(0,len(wholemat),batchsize):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    input = wholemat[:,0:2].float()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    # we feed the inputs into the neural network\n",
        "    #calculate the mean squared error loss between the output of the neural network and the groundtruth\n",
        "    # backpropagate through the neural network using this loss\n",
        "    # update the weights of the neural network using Adam and the loss\n",
        "\n",
        "    # print statistics\n",
        "    if epoch % 10 == 0:    \n",
        "        print(\"training loss\", loss)\n",
        "        net.eval() # this means that we are evaluating the neural network. the validation loss is not used for backpropagation\n",
        "        print(\"validation loss\", torch.mean((net(evalmat[:,0:2].float()) - evalmat[:,2:4])**2))\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "KgAq1gH0IvqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the result\n",
        "\n",
        "Let's plot the output $\\frac{d\\theta}{dt}$ and $\\frac{dv}{dt}$ of the neural network and compare it against the groundtruth $\\frac{d\\theta}{dt}$ and $\\frac{dv}{dt}$."
      ],
      "metadata": {
        "id": "XHRJs-0nmo58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x,y = np.meshgrid(np.linspace(np.min(df['theta'])*1.1, np.max(df['theta'])*1.1,20), np.linspace(np.min(df['v'])*1.1, np.max(df['v'])*1.1,20))\n",
        "net_output_derivatives = net(torch.tensor(np.c_[np.ravel(x), np.ravel(y)]).float()).detach().numpy()\n",
        "dtheta, dv = net_output_derivatives[:,0].reshape(x.shape), net_output_derivatives[:,1].reshape(y.shape)"
      ],
      "metadata": {
        "id": "5GmlWla1Jdte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,10))\n",
        "plt.quiver(x,y,dtheta,dv, color = 'blue', label = 'NN output')\n",
        "plt.quiver(x,y,y,-np.sin(x), color = 'grey', label = 'groundtruth')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "KDxVRoS8p3vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning the total energy of the system"
      ],
      "metadata": {
        "id": "kBSRWN4Sqfh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For physical systems, their total energy $H$ is conserved. We can use this information to train a neural network to evaluate the total energy of a neural network, given $\\theta$ and $v$. This type of system (with total energy that is conserved), is also called a Hamiltonian system [1](https://en.wikipedia.org/wiki/Hamiltonian_system) [2](https://en.wikipedia.org/wiki/Hamiltonian_system). \n",
        "\n",
        "Hamiltonian systems follow the Hamilton's equations:\n",
        "$\\frac{dv}{dt} = -\\frac{\\partial H}{\\partial \\theta}$, $\\frac{d\\theta}{dt} = \\frac{\\partial H}{\\partial v}$. Furthermore, we know that a system at rest with $\\theta=0$ and $v=0$ has a total energy of $H=0$.\n",
        "\n",
        "This is the **knowledge/information** that we have about the Hamiltonian system. We want to *embed this knowledge in the loss function* of a neural network to *constrain the neural network* and bias it toward learning the total energy of our system."
      ],
      "metadata": {
        "id": "ZMh9mFXVrs6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic differentiation relies on a classic calculus formula known as the chain-rule. The chain rule allows us to calculate very complex derivatives by splitting them and recombining them later.\n",
        "\n",
        "Formally speaking, given a composite function $f(g(x))$, we can calculate its derivative as $\\frac{\\partial}{\\partial x} f(g(x)) = f'(g(x))g'(x)$. This result is what makes automatic differentiation work. By combining the derivatives of the simpler functions that compose a larger one, such as a neural network, it is possible to compute the exact value of the gradient at a given point rather than relying on the numerical approximation, which would require multiple perturbations in the input to obtain a value.\n",
        "\n",
        "Using this concept, we can compute the derivative of the output of a neural network $f(x)$ with respect to its inputs $x$"
      ],
      "metadata": {
        "id": "09NxTIIlrvQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Hnet = Net(2,16,1)"
      ],
      "metadata": {
        "id": "Ene-2tVeq27q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(Hnet, mat):\n",
        "  input    = torch.autograd.Variable(mat[:,0:2].float(),requires_grad=True) # requires_grad = True for automatic differentiation\n",
        "  outputs  = Hnet(input) # we feed the inputs into the neural network, and want the output to be the total energy of the system\n",
        "  dH       = torch.autograd.grad(outputs, input, grad_outputs=outputs.data.new(outputs.shape).fill_(1),create_graph=True)[0]\n",
        "  dHdtheta = dH[:,0]\n",
        "  dHdv     = dH[:,1]\n",
        "  dthetadt = mat[:,2]\n",
        "  dvdt     = mat[:,3]\n",
        "\n",
        "  loss1 = torch.mean((dHdv-dthetadt)**2,dim=0)\n",
        "  loss2 = torch.mean((dHdtheta+dvdt)**2,dim=0) \n",
        "  loss3 = (Hnet(torch.tensor([[0.,0.]]).to(device))-torch.tensor([[0.]]).to(device))**2 # a system at rest has 0 energy\n",
        "\n",
        "  return loss1, loss2, loss3"
      ],
      "metadata": {
        "id": "VBGLO7a9tIAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(Hnet.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(500):  # loop over the dataset multiple times\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # calculating the loss\n",
        "    loss1, loss2, loss3 = calculate_loss(Hnet, wholemat)\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    loss = loss1 + loss2 + loss3  #calculate the mean squared error loss between the output of the neural network and the groundtruth\n",
        "    loss.backward() # backpropagate through the neural network using this loss\n",
        "    optimizer.step() # update the weights of the neural network using Adam and the loss\n",
        "\n",
        "    # print statistics\n",
        "    if epoch % 10 == 0:    \n",
        "        print(\"training loss\", loss[0][0])\n",
        "        Hnet.eval()\n",
        "        loss1, loss2, loss3 = calculate_loss(Hnet, evalmat)\n",
        "        print(\"validation loss from each equation: loss1 = %s, loss2 = %s, loss3 = %s\" %(loss1.detach().cpu().item(), loss2.detach().cpu().item(), loss3.detach().cpu().item()))\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "F08EUdDuqhoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the result"
      ],
      "metadata": {
        "id": "et3EcvPlznwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x,y = np.meshgrid(np.linspace(np.min(df['theta'])*1.1, np.max(df['theta'])*1.1,200), np.linspace(np.min(df['v'])*1.1, np.max(df['v'])*1.1,200))\n",
        "net_output_Hamiltonian = Hnet(torch.tensor(np.c_[np.ravel(x), np.ravel(y)]).float()).detach().numpy()\n",
        "H_output = net_output_Hamiltonian.reshape(x.shape)"
      ],
      "metadata": {
        "id": "1wI0xY55zsuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def H_true(x,y):\n",
        "  return 0.5*y**2 + 1 - np.cos(x)"
      ],
      "metadata": {
        "id": "YrSHfp-U2ODn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize = (20,10))\n",
        "\n",
        "im0 = ax[0].imshow(H_output, extent = (np.min(df['theta'])*1.1, np.max(df['theta'])*1.1, np.min(df['v'])*1.1, np.max(df['v'])*1.1), cmap = plt.cm.jet, aspect = 'auto')\n",
        "divider = make_axes_locatable(ax[0])\n",
        "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "ax[0].title.set_text(\"Predicted Hamiltonian using Neural Network\")\n",
        "\n",
        "im1 = ax[1].imshow(H_true(x,y), extent = (np.min(df['theta'])*1.1, np.max(df['theta'])*1.1, np.min(df['v'])*1.1, np.max(df['v'])*1.1), cmap = plt.cm.jet, aspect = 'auto')\n",
        "divider = make_axes_locatable(ax[1])\n",
        "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "fig.colorbar(im1, cax=cax, orientation='vertical')\n",
        "ax[1].title.set_text(\"True Hamiltonian (groundtruth)\")"
      ],
      "metadata": {
        "id": "6b70LZ32zsuC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}