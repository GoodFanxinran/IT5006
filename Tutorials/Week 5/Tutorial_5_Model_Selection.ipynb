{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Z6PDQ-HbgL"
      },
      "source": [
        "# Week 5 - Model Selection\n",
        "\n",
        "## Learning Objectives\n",
        "+ Supervised and unsupervised learning\n",
        "+ Train-Test Split\n",
        "+ Preprocessing and Choice of Model\n",
        "    + Classification using Naive Bayes\n",
        "    + Using Pipeline in Sklearn\n",
        "+ Model Validation \n",
        "    + Evaluation Metrics\n",
        "    \n",
        "For this tutorial, you need the following installed:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "id": "-7wBhBS-fgli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A supervised learning example (regression)\n",
        "\n",
        "We work on the ```sklearn``` iris dataset to identify how sepal length varies with sepal width, petal length and petal width. \n",
        "\n",
        "+ We split the dataset into train and test datasets.\n",
        "+ We fit the linear regression model on the train dataset.\n",
        "+ We use the linear regression model to predict on the test dataset.\n",
        "+ We calculate the mean squared error to understand goodness of fit."
      ],
      "metadata": {
        "id": "1DZmhq7fPLQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(data= iris.data, columns= iris.feature_names)\n",
        "target_df = pd.DataFrame(data= iris.target, columns= ['species'])\n",
        "iris_df = pd.concat([iris_df, target_df], axis= 1)\n",
        "iris_df.head()"
      ],
      "metadata": {
        "id": "72KfyP7WPSBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_df.describe()"
      ],
      "metadata": {
        "id": "Ix8bF0fCP_Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "sns.pairplot(iris_df, hue= 'species')"
      ],
      "metadata": {
        "id": "cTJQuZC3QQed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is good practice to scale the dataset using ```StandardScaler()``` but since data preprocessing is not the focus of this tutorial, and the magnitude of all features is the same, we skip this step.\n",
        "\n",
        "We split the dataset using ```train_test_split``` from ```sklearn```.\n",
        "\n",
        "Whenever randomization is part of a Scikit-learn algorithm, a ```random_state``` parameter may be provided to control the random number generator used. In order to obtain reproducible (i.e. constant) results across multiple program executions, we need to remove all uses of ```random_state=None```, which is the default. The recommended way in sklearn is to declare a ```rng``` variable at the top of the program, and pass it down to any object that accepts a ```random_state``` parameter."
      ],
      "metadata": {
        "id": "CB380o7YUhlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X= iris_df[['petal width (cm)', 'petal length (cm)', 'sepal width (cm)']].values\n",
        "y= iris_df['sepal length (cm)'].values\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "rng = np.random.RandomState(0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.33, random_state= rng)"
      ],
      "metadata": {
        "id": "OZhrIzKcR0w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform a linear regression to fit the train dataset. How do we find the $R^2$?"
      ],
      "metadata": {
        "id": "lXBdWCWtgqlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "uIW6fy4ZSnQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the coefficients that give this $R^2$?"
      ],
      "metadata": {
        "id": "ZX7UbOxzg5Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NfH9n07rg2TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the fitted linear regression model to perform predictions in the test dataset. We can also calculate metrics using the predictions. "
      ],
      "metadata": {
        "id": "s7vDAnGIhEkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr.predict(X_test)\n",
        "pred = lr.predict(X_test)"
      ],
      "metadata": {
        "id": "ew2RYFl4S1cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print('Mean Squared Error:', mean_squared_error(y_test, pred))\n",
        "print('Mean Root Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))"
      ],
      "metadata": {
        "id": "D-3KBMLhTNjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An unsupervised learning example\n",
        "\n",
        "We work on the ```sklearn``` iris dataset to identify how petal length and petal width vary with species. We use KMeans"
      ],
      "metadata": {
        "id": "PT-8SYz8Ubpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X= iris_df[['petal width (cm)', 'petal length (cm)']].values\n",
        "y= iris_df['species'].values"
      ],
      "metadata": {
        "id": "jORAki_ihMjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do I do a train test split? "
      ],
      "metadata": {
        "id": "7Fmg5f67rBRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "km = KMeans(n_clusters = 3, random_state=rng)\n",
        "km.fit(X)"
      ],
      "metadata": {
        "id": "_4LsW5Zop_GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_labels = km.labels_\n",
        "iris_df['species_predict'] = iris_labels"
      ],
      "metadata": {
        "id": "DgYLq6Qsmvnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axs = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw=dict(width_ratios=[4, 4]))\n",
        "sns.scatterplot(data=iris_df, x=\"petal width (cm)\", y=\"petal length (cm)\", hue=\"species\", ax=axs[0])\n",
        "sns.scatterplot(data=iris_df, x=\"petal width (cm)\", y=\"petal length (cm)\", hue=\"species_predict\", ax=axs[1])\n",
        "f.tight_layout()"
      ],
      "metadata": {
        "id": "1mzqyvJmp8ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find the optimal number of clusters using the elbow method."
      ],
      "metadata": {
        "id": "Qe_ZNqvZroXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sum_of_squared_distances = []\n",
        "K = range(1,15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(X)\n",
        "    Sum_of_squared_distances.append(km.inertia_)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fkgceyVcq9rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJTDMgYKHbgW"
      },
      "source": [
        "# A supervised learning example (classification) on the Spam Detection dataset\n",
        "\n",
        "Let's try to handle a new type of data- text data\n",
        "\n",
        "Given a text document, we want to be able to classify whether it is a spam or not (binary classification). We use the SMS Spam dataset available in this [kaggle competition](https://www.kaggle.com/uciml/sms-spam-collection-dataset).\n",
        "\n",
        "The data is available as a csv in which the first column is the class label. The \"spam\" label refers to message being categorized as spam, while \"ham\" label exists when the SMS is not a spam.\n",
        "\n",
        "Let us first load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeqQrDRQHbgX"
      },
      "outputs": [],
      "source": [
        "sms = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IT5006/Week 5/spam.csv', encoding='latin-1')\n",
        "sms.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "sftJmGyyHbgc"
      },
      "outputs": [],
      "source": [
        "sms = sms.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\n",
        "sms = sms.rename(columns = {'v1':'label','v2':'message'})\n",
        "sms.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4H3I3-dHbge"
      },
      "outputs": [],
      "source": [
        "sms.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Eb4c2bHbgj"
      },
      "source": [
        "# Train-Test split\n",
        "\n",
        "Before we begin our modeling, let us first split the data into train and test split. For this, we can use the [```train_test_split```](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) utility available in sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFalcV4fHbgk"
      },
      "outputs": [],
      "source": [
        "sms_train, sms_test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZiZ5oQYHbgl"
      },
      "outputs": [],
      "source": [
        "print(\"No of samples in train set: %s\"%(len(sms_train)))\n",
        "print(\"No of samples in test set: %s\"%(len(sms_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAVY5qQfHbgu"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Let us first get quick descriptive statistics of the data. As the aim of the tutorial is not preprocessing, we will do quick operations and majorly focus on handling text data and learning to train a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dJJJQFiPHbgv"
      },
      "outputs": [],
      "source": [
        "sms_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KFVdLHacHbgw"
      },
      "outputs": [],
      "source": [
        "sms_train.groupby('label').describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Gt_DN5AdHbgy"
      },
      "outputs": [],
      "source": [
        "sms_train['length'] = sms_train['message'].apply(len)\n",
        "sms_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-2N1NHOHbg0"
      },
      "source": [
        "## Punctuation and Stopword Removal\n",
        "\n",
        "Stopword refers to commonly used words, such as \"a\", \"the\", \"is\", etc. These words are not providing very useful information and hence are generally removed during preprocessing.\n",
        "\n",
        "Nltk library has a list of stopwords. We can use this list to filter out the stopwords from our documents. However, we must be careful about using all these preprocessing steps, and decide based on the data and task what preprocessing to perform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5ny7SCHbg0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def wordCount(text, testing=False): \n",
        "    try:\n",
        "        text = text.lower() # convert text to lower case\n",
        "        if testing==True:\n",
        "            print(text)\n",
        "        regex = re.compile('['+re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n'+']')\n",
        "        txt = regex.sub('',text)  # remove punctuation\n",
        "        if testing==True:\n",
        "            print(txt)\n",
        "        words = [w for w in txt.split(' ')\n",
        "                if w not in stop] # remove stop words and words with length smaller than 3 letters. create array of remaining words\n",
        "        if testing==True:\n",
        "            print(words)\n",
        "        return len(words)\n",
        "    except:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YY7pA9CVHbg2"
      },
      "outputs": [],
      "source": [
        "wordCount(sms_train['message'].iloc[103], testing=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI6pDDvsHbg2"
      },
      "source": [
        "Let us now create two new features for the word length of the message and the processed word length. The processed word length is essentually just going to be all words in message sans the stopwords."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use df.apply to count the number of words in each message."
      ],
      "metadata": {
        "id": "riOEBeUZ2xsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0Nx06N0E2MUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "QZLzV-fQHbg4"
      },
      "outputs": [],
      "source": [
        "sms_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLHXy4CxHbg_"
      },
      "source": [
        "As we have done the preprocessing on the train set, we need to do the feature generation similarly for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "7IL6nzoVHbhA"
      },
      "outputs": [],
      "source": [
        "sms_test['length'] = sms_test['message'].apply(len)\n",
        "sms_test['word_length'] = sms_test['message'].apply(lambda x: len([w for w in x.split(' ')]))\n",
        "sms_test['processed_word_length'] = sms_test['message'].apply(lambda x: wordCount(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CTyZcq1SHbhG"
      },
      "outputs": [],
      "source": [
        "x_train = sms_train[['length', 'word_length', 'processed_word_length']].to_numpy()\n",
        "x_test = sms_test[['length', 'word_length', 'processed_word_length']].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also change the spam/ ham labels to numeric."
      ],
      "metadata": {
        "id": "mkejsx4C0ute"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWet1yzkHbhJ"
      },
      "outputs": [],
      "source": [
        "y_train = [1 if l==\"spam\" else 0 for l in sms_train['label']]\n",
        "y_test = [1 if l==\"spam\" else 0 for l in sms_test['label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC_ghUJCHbhK"
      },
      "source": [
        "## CountVectorizer in sklearn\n",
        "\n",
        "Sklearn includes a submodule which is dedicated to feature extraction from  [images](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.image) and [text](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text). A useful and simple utility in the text submodule is the [```CountVectorizer```](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer). It includes text preprocessing (punctuation removal and optional stopwords removal and tokenization), builds a dictionary of features (the vocabulary) and transforms documents to feature vectors. This also has option to specify n-gram text consideration,  in case you are interested in more sophisticated analysis. For this tutorial, we will just generate a word count vector based on the vocabulary constructed. \n",
        "\n",
        "![CountVectorizer](https://www.educative.io/api/edpresso/shot/5197621598617600/image/6596233398321152)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp-l8CMqHbhK"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(sms_train.message)\n",
        "print(\"No of samples in train set: %s\"%(len(sms_train)))\n",
        "X_train_counts.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What features are in our dataset?"
      ],
      "metadata": {
        "id": "dEnj-oF_57aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IcyhcGJUd7Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sms_train.message[200]"
      ],
      "metadata": {
        "id": "hDWAuWMGeRmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_counts.toarray()[200]"
      ],
      "metadata": {
        "id": "UXCmOkuHe7mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noWCljOiHbhM"
      },
      "outputs": [],
      "source": [
        "X_test_counts = count_vect.transform(sms_test.message)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## REMOVE what happens to words in the test dataset that have not been seen before (tags: unseen)\n",
        "np.sum(count_vect.transform([\"arts\"]).toarray())"
      ],
      "metadata": {
        "id": "pIL_0dRz6tyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ov2j4GYkHbhN"
      },
      "outputs": [],
      "source": [
        "print(X_train_counts.shape)\n",
        "print(x_train.shape)\n",
        "print(X_test_counts.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IBmEF0lHbhN"
      },
      "source": [
        "Let us now create the dataset by concatenating the word length features and the word count vector. Do note that essentially the word count vector is also providing us information regarding the count of the words - which is similar to the length. We do not really expect to see huge improvements with this approach. But we are continuing in this tutorial so as to learn about mixing and using such differently created features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY4P0q9LHbhO"
      },
      "outputs": [],
      "source": [
        "trainData = np.hstack((X_train_counts.todense(), x_train))\n",
        "testData = np.hstack((X_test_counts.todense(), x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTdSUm_9HbhO"
      },
      "source": [
        "# Classification using Naive Bayes Algorithm\n",
        "\n",
        "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. \n",
        "\n",
        "If *y* is the prediction, and the *x*s are the features, then Bayes' theorem gives the conditional probability of y given x. Using the conditional assumption among features, the equations are simplified to provide us an estimate of *y*. The different Naive Bayes algorithms typically differ in the assumption of the distribution of feature given the *y*.\n",
        "\n",
        "In this tutorial, we do not aim to understand a specific classifier, or its working. The aim is to understand how we can experiment with the features and perform predictions using sklearn. Once how to implement is understood, the classifiers in sklearn can be changed according to the problem at hand. \n",
        "\n",
        "In this tutorial, we will try with two different Naive Bayes algorithms available in sklearn. The [User Guide](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes) is a useful resource for finding simple explanation regarding what can be used. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqN7-dFtHbhP"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "\n",
        "clf = MultinomialNB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttzPg4H0HbhQ"
      },
      "outputs": [],
      "source": [
        "clf.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gwMzZxXHbhR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
        "\n",
        "def evaluate(y_pred, y_test):\n",
        "    print(\"###########\")\n",
        "\n",
        "    c = confusion_matrix(y_test, y_pred) #[[TN FP],[FN,TP]]\n",
        "    tn, fp, fn, tp = c.ravel() # returns a flattened array\n",
        "\n",
        "    print(c)\n",
        "    print(\"###########\")\n",
        "    print(\"Accuracy:\"+str(accuracy_score(y_test, y_pred)))\n",
        "    sens, spec = tp/(tp+fn), tn/(tn+fp) \n",
        "    print(\"Specificity:{0}, Sensitivity: {1}\".format(spec, sens))\n",
        "    print(\"Precision:\"+str(precision_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_zFPbBcPHbhR"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "evaluate(y_pred, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p20hqec8HbhS"
      },
      "outputs": [],
      "source": [
        "clf = ComplementNB()\n",
        "\n",
        "clf.fit(trainData, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "r7sLyewwHbhS"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(testData)\n",
        "\n",
        "evaluate(y_pred, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4gOXBn4HbhS"
      },
      "source": [
        "# Putting it all together - Building a pipeline in sklearn\n",
        "\n",
        "We have already seen the ```ColumnTransformer``` in sklearn. Also, we know the naming conventions in sklearn, and have a vague idea about how sklearn makes our life easy in putting the earlier blocks together for experimentation. [```Pipeline```](https://scikit-learn.org/stable/modules/compose.html#pipeline) in sklearn can be used for chaining different estimators together. When we have a fixed sequence of operations, this is usually helpful to put it all together. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLqM82g0HbhT"
      },
      "source": [
        "However, for that we would need the operations to also be in form of estimators. We can easily do so by using existing sklearn API, or writing our custom transformer if we have done our custom preprocessing.\n",
        "\n",
        "## Writing Custom Transformer\n",
        "\n",
        "You can implement a transformer from an arbitrary function with [```FunctionTransformer```](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer). However, if do not have a specific function to implement as transformer, but want flexibility to implement our operations, we can write our transformer using two baseclasses from sklearn: \n",
        "1. [```BaseEstimator```](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html): The estimator provides for get_params and set_params functions. \n",
        "2. [```TransformerMixin```](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html): This class essentially provides us with fit_transform function when we define our own fit and transform functions.\n",
        "\n",
        "In general, it is good to note that all estimators should specify all the parameters that can be set at the class level in their ```__init__``` as explicit keyword arguments. However, for our transformation, we are not storing some transformer parameter, and hence can also skip the ```__init__``` function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an transformer that creates the ```length```, ```word_length``` and ```processed_word_length``` features in the dataframe."
      ],
      "metadata": {
        "id": "xx8U6_vtB-nP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9bghWMlHbhT"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class FeatureCreator( BaseEstimator, TransformerMixin ):    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        self.df = pd.DataFrame()\n",
        "        self.df['length'] = X.apply(len)\n",
        "        self.df['word_length'] = X.apply(wordLength)\n",
        "        self.df['processed_word_length'] = X.apply(wordCount)\n",
        "        return self.df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKhUkb8QHbhU"
      },
      "outputs": [],
      "source": [
        "proc1 = FeatureCreator()\n",
        "x_train = proc1.fit_transform(sms_train.message)\n",
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWJIqrZTHbhW"
      },
      "outputs": [],
      "source": [
        "class DenseTransformer(TransformerMixin):\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None, **fit_params):\n",
        "        return X.todense()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcAvzmQiHbhW"
      },
      "source": [
        "## Preprocessing using FeatureUnion \n",
        "\n",
        "[```FeatureUnion```](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion) combines several transformer objects into a new transformer that combines their output. A ```FeatureUnion``` takes a list of transformer objects. During fitting, each of these is fit to the data **independently**. The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix.\n",
        "\n",
        "Do note here that the each transformer object is fit to the entire data. If you want to specify different transformer for different column - you can go back to the [```ColumnTransformer```](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer) covered in previous tutorial. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgFV2xXHHbhX"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "\n",
        "tf_pipeline = Pipeline([(\"countvec\", count_vect), (\"to_dense\",DenseTransformer())])\n",
        "feats = FeatureUnion([(\"lengths\", proc1), (\"tf\", tf_pipeline)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64F8fK7gHbhY"
      },
      "outputs": [],
      "source": [
        "feats.fit(sms_train.message)\n",
        "x_train = feats.transform(sms_train.message)\n",
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What should we do with the test dataset?"
      ],
      "metadata": {
        "id": "U4lJhRrTEFi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "metadata": {
        "id": "OTzZ47o3EE6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "id": "anLSnNwKdWY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCGAXfdrHbha"
      },
      "outputs": [],
      "source": [
        "text_clf = Pipeline([('feats', feats),('clf', clf)])\n",
        "\n",
        "text_clf.fit(sms_train.message, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRGE68G1Hbhd"
      },
      "outputs": [],
      "source": [
        "y_pred = text_clf.predict(sms_test.message)\n",
        "evaluate(y_pred, y_test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "Tutorial 5 Model Selection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}